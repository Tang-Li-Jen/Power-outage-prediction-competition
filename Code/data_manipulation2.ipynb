{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shapefile #pip install pyshp\n",
    "from geopy.distance import vincenty\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/Users/Qoo/Desktop/taipower/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import observation stations' lat-lon data\n",
    "obs_station = pd.read_csv(path + 'obs_station/obs_intro.csv',encoding='big5',usecols=[0,1,2,3,4,5,6])\n",
    "old_names = list(obs_station.columns)\n",
    "new_names = ['stno','stname','st_height','st_lon','st_lat','st_city','st_address']\n",
    "obs_station.rename(columns=dict(zip(old_names,new_names)),inplace=True)\n",
    "obs_station.stno = obs_station.stno.str.encode('big5')\n",
    "\n",
    "# delete unused obs station\n",
    "obs_station = obs_station[0:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: stno, dtype: int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import observation stations' rainfall data\n",
    "all_files = glob.glob(os.path.join(path+'typhoon_rain/','*.csv'))\n",
    "for file in all_files:\n",
    "    try:\n",
    "        pd.read_csv(file,encoding='big5',usecols=[0,1,2,3])\n",
    "    except:\n",
    "        print 'error at %s' %file\n",
    "\n",
    "tp_rain = pd.concat(pd.read_csv(file,encoding='big5') for file in all_files )\n",
    "tp_rain['stno'] = tp_rain['stno'].astype('str')\n",
    "tp_rain['typhoon'] =  tp_rain['typhoon_name'].str[4:]\n",
    "tp_rain.accu_end_time = pd.to_datetime(tp_rain.accu_end_time)\n",
    "tp_rain['day'] = tp_rain.accu_end_time.dt.day\n",
    "\n",
    "tp_rain.loc[tp_rain.typhoon =='MERANTI','typhoon'] = 'MERANTIANDMALAKAS'\n",
    "tp_rain.loc[tp_rain.typhoon =='MALAKAS','typhoon'] = 'MERANTIANDMALAKAS'\n",
    "tp_rain.loc[tp_rain.typhoon =='NESAT','typhoon'] = 'NESATANDHAITANG'\n",
    "tp_rain.loc[tp_rain.typhoon =='HAITANG','typhoon'] = 'NESATANDHAITANG'\n",
    "\n",
    "# check unmatch station\n",
    "tp_rain2 = pd.merge(tp_rain,obs_station,how='left',on='stno')\n",
    "key = tp_rain2.stname.isnull()\n",
    "tp_rain2[key].stno.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Taiwan village lat-lon dataset\n",
    "def read_shapefile(shp_path):\n",
    "\t\"\"\"\n",
    "\tRead a shapefile into a Pandas dataframe with a 'coords' column holding\n",
    "\tthe geometry information. This uses the pyshp package\n",
    "\t\"\"\"\n",
    "\timport shapefile\n",
    "\n",
    "\t#read file, parse out the records and shapes\n",
    "\tsf = shapefile.Reader(shp_path)\n",
    "\tfields = [x[0] for x in sf.fields][1:]\n",
    "\trecords = sf.records()\n",
    "\tshps = [s.points for s in sf.shapes()]\n",
    "\n",
    "\t#write into a dataframe\n",
    "\tdf = pd.DataFrame(columns=fields, data=records)\n",
    "\tdf = df.assign(coords=shps)\n",
    "\n",
    "\treturn df\n",
    "\n",
    "vil_latlon = read_shapefile(path + \"village_latlon/VILLAGE_MOI_1060831.shp\")\n",
    "\n",
    "# compute village central point\n",
    "for element in range(0,len(vil_latlon)):\n",
    "    vil_latlon.loc[element,'vil_lon'] = pd.DataFrame(vil_latlon.coords[element]).mean()[0]\n",
    "    vil_latlon.loc[element,'vil_lat'] = pd.DataFrame(vil_latlon.coords[element]).mean()[1]\n",
    "vil_latlon = vil_latlon.iloc[:,[0,1,2,3,11,12]]\n",
    "\n",
    "# impute missing village & delete unused village\n",
    "vil_latlon.loc[0,['VILLCODE','VILLNAME']] = ['10013030023','大鵬里']\n",
    "vil_latlon = vil_latlon.loc[vil_latlon.VILLNAME !='',:]\n",
    "vil_latlon = vil_latlon.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import df generated from data_manipulation1\n",
    "df = pd.read_csv(path + 'df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VILLCODE</th>\n",
       "      <th>COUNTYNAME</th>\n",
       "      <th>TOWNNAME</th>\n",
       "      <th>VILLNAME</th>\n",
       "      <th>vil_lon</th>\n",
       "      <th>vil_lat</th>\n",
       "      <th>stno</th>\n",
       "      <th>stname</th>\n",
       "      <th>st_height</th>\n",
       "      <th>st_lon</th>\n",
       "      <th>st_lat</th>\n",
       "      <th>st_city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [VILLCODE, COUNTYNAME, TOWNNAME, VILLNAME, vil_lon, vil_lat, stno, stname, st_height, st_lon, st_lat, st_city]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pair each village with its nearest observation station\n",
    "obs_station.loc[:,'st_lonlat'] = obs_station.loc[:,['st_lon','st_lat']].apply(tuple,axis=1)\n",
    "vil_latlon.loc[:,'vil_lonlat'] = vil_latlon.loc[:,['vil_lon','vil_lat']].apply(tuple,axis=1)\n",
    "\n",
    "# find the closet obs station to village\n",
    "def closest(point, points):\n",
    "    distance = [vincenty(point, i).miles for i in points]\n",
    "    return distance.index(min(distance))\n",
    "\n",
    "for i in range(0,len(vil_latlon)):\n",
    "    vil_latlon.loc[i,'stno_index'] = closest(vil_latlon.loc[i,'vil_lonlat'], obs_station.loc[:,'st_lonlat'])\n",
    "obs_station.loc[:,'stno_index'] = obs_station.index\n",
    "vil_latlon2 = pd.merge(vil_latlon, obs_station, how='left', on='stno_index')\n",
    "vil_latlon2 = vil_latlon2.iloc[:,[0,1,2,3,4,5,8,9,10,11,12,13]]\n",
    "vil_latlon2.to_csv(path +'village_intro.csv',encoding='utf-8')\n",
    "\n",
    "# check unmatch village and obs station\n",
    "key = vil_latlon2.stno.isnull()\n",
    "vil_latlon2[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with ViLCode\n",
    "for i in range(0,len(vil_latlon2)):\n",
    "    if vil_latlon2.loc[i,'VILLCODE'][3:5] =='00':\n",
    "        vil_latlon2.loc[i,'VilCode'] = vil_latlon2.loc[i,'VILLCODE'][0:3] +  vil_latlon2.loc[i,'VILLCODE'][5:7] + '00-'+  vil_latlon2.loc[i,'VILLCODE'][8:11]\n",
    "    else:\n",
    "        vil_latlon2.loc[i,'VilCode'] = vil_latlon2.loc[i,'VILLCODE'][0:7] +  '-'+  vil_latlon2.loc[i,'VILLCODE'][8:11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.merge(df,vil_latlon2,how='left', on='VilCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CityName</th>\n",
       "      <th>TownName</th>\n",
       "      <th>VilName</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [CityName, TownName, VilName, 0]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find unmatch region\n",
    "key = df2.stno.isnull()\n",
    "miss = df2[key]\n",
    "miss.groupby(['CityName','TownName','VilName']).size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# df 多出 大鵬里\n",
    "print set(df.VilCode) - set(vil_latlon2.VilCode)\n",
    "#df.loc[df.VilCode=='1001303-023',:]\n",
    "\n",
    "# vil_latlon 多出 中崙里\n",
    "print set(vil_latlon2.VilCode) - set(df.VilCode)\n",
    "vil_latlon2.loc[vil_latlon2.VilCode =='1000401-031',:]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "33\n",
      "set(['467790', '466850'])\n",
      "set([])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stno</th>\n",
       "      <th>stname</th>\n",
       "      <th>st_height</th>\n",
       "      <th>st_lon</th>\n",
       "      <th>st_lat</th>\n",
       "      <th>st_city</th>\n",
       "      <th>st_address</th>\n",
       "      <th>st_lonlat</th>\n",
       "      <th>stno_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>466850</td>\n",
       "      <td>五分山雷達站</td>\n",
       "      <td>756.0</td>\n",
       "      <td>121.7812</td>\n",
       "      <td>25.0712</td>\n",
       "      <td>新北市</td>\n",
       "      <td>瑞芳區靜安路四段1巷1號</td>\n",
       "      <td>(121.7812, 25.0712)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>467790</td>\n",
       "      <td>墾丁雷達站</td>\n",
       "      <td>42.0</td>\n",
       "      <td>120.8080</td>\n",
       "      <td>21.9482</td>\n",
       "      <td>屏東縣</td>\n",
       "      <td>恆春鎮燈塔路51巷33號</td>\n",
       "      <td>(120.808, 21.9482)</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      stno  stname  st_height    st_lon   st_lat st_city    st_address  \\\n",
       "0   466850  五分山雷達站      756.0  121.7812  25.0712     新北市  瑞芳區靜安路四段1巷1號   \n",
       "30  467790   墾丁雷達站       42.0  120.8080  21.9482     屏東縣  恆春鎮燈塔路51巷33號   \n",
       "\n",
       "              st_lonlat  stno_index  \n",
       "0   (121.7812, 25.0712)           0  \n",
       "30   (120.808, 21.9482)          30  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether all obs station have rainfall records\n",
    "print len(np.unique(tp_rain.stno))\n",
    "print len(np.unique(df2.stno))\n",
    "print set(df2.stno) - set(tp_rain.stno)\n",
    "print set(tp_rain.stno) - set(df2.stno)\n",
    "obs_station.loc[(obs_station.stno =='467790')|(obs_station.stno =='466850'),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean hourly rainfall\n",
    "mean_hour_rain = tp_rain.groupby(['typhoon','stno'], as_index=False).agg({'accu_value':'mean'})\n",
    "mean_hour_rain.columns.values[2] = 'mean_accu_hour_rain'\n",
    "\n",
    "# mean daily rainfall\n",
    "mean_day_rain = tp_rain.groupby(['day','typhoon','stno'], as_index=False).agg({'accu_value':'sum'})\n",
    "mean_day_rain = mean_day_rain.groupby(['typhoon','stno'], as_index=False).agg({'accu_value':'mean'})\n",
    "mean_day_rain.columns.values[2] = 'mean_accu_day_rain'\n",
    "\n",
    "# accu rainfall\n",
    "total_rain = tp_rain.groupby(['typhoon','stno'], as_index=False).agg({'accu_value':'sum'})\n",
    "total_rain.columns.values[2] = 'accu_rain'\n",
    "\n",
    "#達到幾次大雨指標(每日累積降雨 80MM以上)\n",
    "# number of times daily accu rainfall > 80 mm\n",
    "heavy_rain = tp_rain.groupby(['day','typhoon','stno'], as_index=False).agg({'accu_value':'sum'})\n",
    "heavy_rain_times = heavy_rain.loc[heavy_rain.accu_value >= 80,:].groupby(['typhoon','stno'], \n",
    "                as_index=False).agg({'accu_value':'count'})\n",
    "heavy_rain_times.columns.values[2] = 'heavy_rain_count_rule1'\n",
    "\n",
    "#達到幾次大雨指標(每小時累積降雨 40MM以上)\n",
    "# number of times hourly accu rainfall > 40 mm\n",
    "heavy_rain_times2 = tp_rain.loc[tp_rain.accu_value >=40,:].groupby(['typhoon','stno'],as_index=False).agg({'accu_value':'count'})\n",
    "heavy_rain_times2.columns.values[2] = 'heavy_rain_count_rule2'\n",
    "\n",
    "#達到幾次豪雨指標(每日累積降雨 200MM以上)\n",
    "# number of times daily accu rainfall > 200 mm\n",
    "how_rain = tp_rain.groupby(['day','typhoon','stno'], as_index=False).agg({'accu_value':'sum'})\n",
    "how_rain_times1 = how_rain.loc[how_rain.accu_value >= 200,:].groupby(['typhoon','stno'], \n",
    "                as_index=False).agg({'accu_value':'count'})\n",
    "how_rain_times1.columns.values[2] = 'how_rain_count_rule1'\n",
    "\n",
    "#達到幾次大豪雨指標(每日累積降雨 350MM以上)\n",
    "# number of times daily accu rainfall > 350 mm\n",
    "big_how_rain = tp_rain.groupby(['day','typhoon','stno'], as_index=False).agg({'accu_value':'sum'})\n",
    "big_how_rain_times = big_how_rain.loc[big_how_rain.accu_value >= 350,:].groupby(['typhoon','stno'], \n",
    "                as_index=False).agg({'accu_value':'count'})\n",
    "big_how_rain_times.columns.values[2] = 'big_how_rain_count'\n",
    "\n",
    "#達到幾次超大豪雨指標(每日累積降雨 500MM以上)\n",
    "# number of times daily accu rainfall > 500 mm\n",
    "big_big_how_rain = tp_rain.groupby(['day','typhoon','stno'], as_index=False).agg({'accu_value':'sum'})\n",
    "big_big_how_rain_times = big_big_how_rain.loc[big_big_how_rain.accu_value >= 500,:].groupby(['typhoon','stno'], \n",
    "                as_index=False).agg({'accu_value':'count'})\n",
    "big_big_how_rain_times.columns.values[2] = 'big_big_how_rain_count'\n",
    "\n",
    "#不同颱風在各測站下的每小時最大累積雨量\n",
    "# max hourly rainfall\n",
    "max_hour_rain = tp_rain.groupby(['typhoon','stno'], as_index=False).agg({'accu_value':'max'})\n",
    "max_hour_rain.columns.values[2] = 'max_hour_accu_rain'\n",
    "#不同颱風在各測站下的每日最大累積雨量\n",
    "# max daily accu rainfall\n",
    "max_day_rain = tp_rain.groupby(['day','typhoon','stno'], as_index=False).agg({'accu_value':'sum'}).groupby(['typhoon','stno'], as_index=False).agg({'accu_value':'max'})\n",
    "max_day_rain.columns.values[2] = 'max_day_accu_rain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.merge(df2, mean_hour_rain, on=['stno','typhoon'],how ='left')\n",
    "df2 = pd.merge(df2, mean_day_rain, on=['stno','typhoon'],how ='left')\n",
    "df2 = pd.merge(df2, total_rain, on=['stno','typhoon'],how ='left')\n",
    "df2 = pd.merge(df2, heavy_rain_times, on=['stno','typhoon'],how ='left')\n",
    "df2 = pd.merge(df2, heavy_rain_times2, on=['stno','typhoon'],how ='left')\n",
    "df2 = pd.merge(df2, how_rain_times1, on=['stno','typhoon'],how ='left')\n",
    "df2 = pd.merge(df2, big_how_rain_times, on=['stno','typhoon'],how ='left')\n",
    "df2 = pd.merge(df2, big_big_how_rain_times, on=['stno','typhoon'],how ='left')\n",
    "df2 = pd.merge(df2, max_hour_rain, on=['stno','typhoon'],how ='left')\n",
    "df2 = pd.merge(df2, max_day_rain, on=['stno','typhoon'],how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing value\n",
    "# note: 1. as to two obs station don't have rainfall records. 2. various rainfall index\n",
    "df3 = pd.concat([ df2.iloc[:,0:55], df2.iloc[:,55:65].fillna(0)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import obs stations' typhoon wind data \n",
    "all_files = glob.glob(os.path.join(path+'typhoon_wind/','*.csv'))\n",
    "for file in all_files:\n",
    "    try:\n",
    "        pd.read_csv(file,encoding='big5',usecols=[0,1,2,3])\n",
    "    except:\n",
    "        print 'error at %s' %file\n",
    "\n",
    "tp_wind = pd.concat(pd.read_csv(file,encoding='big5') for file in all_files )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperately process on typhoon max wind speed  & wind gust\n",
    "tp_wm = tp_wind.iloc[:,[0,2,4,5,6,7,8]]\n",
    "tp_wg = tp_wind.iloc[:,[0,1,3,5,6,7,8]]\n",
    "\n",
    "# drop unreasonable data\n",
    "tp_wm = tp_wm.loc[tp_wm.WSMax >0,]\n",
    "tp_wg = tp_wg.loc[tp_wg.WSGust >0,]\n",
    "\n",
    "# preprocess\n",
    "tp_wm['stno'] = tp_wm['stno'].astype('str')\n",
    "tp_wm['typhoon'] = tp_wm.typhoon_name.str[4:]\n",
    "tp_wm.ObsTime = pd.to_datetime(tp_wm.ObsTime)\n",
    "tp_wm['day'] = tp_wm.ObsTime.dt.day\n",
    "tp_wm.loc[tp_wm.typhoon =='MERANTI','typhoon'] = 'MERANTIANDMALAKAS'\n",
    "tp_wm.loc[tp_wm.typhoon =='MALAKAS','typhoon'] = 'MERANTIANDMALAKAS'\n",
    "tp_wm.loc[tp_wm.typhoon =='NESAT','typhoon'] = 'NESATANDHAITANG'\n",
    "tp_wm.loc[tp_wm.typhoon =='HAITANG','typhoon'] = 'NESATANDHAITANG'\n",
    "\n",
    "tp_wg['stno'] = tp_wg['stno'].astype('str')\n",
    "tp_wg['typhoon'] = tp_wg.typhoon_name.str[4:]\n",
    "tp_wg.ObsTime = pd.to_datetime(tp_wg.ObsTime)\n",
    "tp_wg['day'] = tp_wg.ObsTime.dt.day\n",
    "tp_wg.loc[tp_wg.typhoon =='MERANTI','typhoon'] = 'MERANTIANDMALAKAS'\n",
    "tp_wg.loc[tp_wg.typhoon =='MALAKAS','typhoon'] = 'MERANTIANDMALAKAS'\n",
    "tp_wg.loc[tp_wg.typhoon =='NESAT','typhoon'] = 'NESATANDHAITANG'\n",
    "tp_wg.loc[tp_wg.typhoon =='HAITANG','typhoon'] = 'NESATANDHAITANG'\n",
    "\n",
    "tp_wm.loc[tp_wm.WDMax ==550,'WDMax'] = 190\n",
    "tp_wm.loc[tp_wm.WDMax ==999.9,'WDMax'] = 279.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean hourly max wind speed\n",
    "#平均每小時最大風速\n",
    "mean_hr_wsm = tp_wm.groupby(['typhoon','stno'], as_index=False).agg({'WSMax':'mean'})\n",
    "mean_hr_wsm.columns.values[2] = 'mean_hr_wsmax'\n",
    "\n",
    "# mean hourly wind gust\n",
    "#平均每小時瞬間極大風速\n",
    "mean_hr_wsg = tp_wg.groupby(['typhoon','stno'], as_index=False).agg({'WSGust':'mean'})\n",
    "mean_hr_wsg.columns.values[2] = 'mean_hr_wsgust'\n",
    "\n",
    "# max hourly max wind speed\n",
    "#最大小時風速\n",
    "max_hr_wsm = tp_wm.groupby(['typhoon','stno'], as_index=False).agg({'WSMax':'max'})\n",
    "max_hr_wsm.columns.values[2] = 'max_hr_wsmax'\n",
    "\n",
    "# max hourly wind gust\n",
    "#最大小時瞬間極大風速\n",
    "max_hr_wsg = tp_wg.groupby(['typhoon','stno'], as_index=False).agg({'WSGust':'max'})\n",
    "max_hr_wsg.columns.values[2] = 'max_hr_wsgust'\n",
    "\n",
    "# the number of times max wind speed > 20.8 \n",
    "#測站測得颱風最大風速每小時達到烈風以上的總次數\n",
    "total_daily_sum_WSM = tp_wm.loc[tp_wm.WSMax >20.8,:].groupby(['typhoon','stno'], as_index=False).agg({'WSMax':'count'})\n",
    "total_daily_sum_WSM.columns.values[2] = 'total_daily_sum_WSMax'\n",
    "\n",
    "# the number of times wind gust > 20.8\n",
    "#測站測得颱風瞬間極大風速每小時達到烈風以上的總次數\n",
    "total_daily_sum_WSG =tp_wg.loc[tp_wg.WSGust >20.8,:].groupby(['typhoon','stno'], as_index=False).agg({'WSGust':'count'})\n",
    "total_daily_sum_WSG.columns.values[2] = 'total_daily_sum_WSGust'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(df3, mean_hr_wsm, on=['stno','typhoon'],how ='left')\n",
    "df3 = pd.merge(df3, mean_hr_wsg, on=['stno','typhoon'],how ='left')\n",
    "df3 = pd.merge(df3, max_hr_wsm, on=['stno','typhoon'],how ='left')\n",
    "df3 = pd.merge(df3, max_hr_wsg, on=['stno','typhoon'],how ='left')\n",
    "df3 = pd.merge(df3, total_daily_sum_WSM, on=['stno','typhoon'],how ='left')\n",
    "df3 = pd.merge(df3, total_daily_sum_WSG, on=['stno','typhoon'],how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing value\n",
    "df3[['mean_hr_wsmax',\n",
    "       'mean_hr_wsgust', 'max_hr_wsmax', 'max_hr_wsgust',\n",
    "       'total_daily_sum_WSMax', 'total_daily_sum_WSGust']] = df3[['mean_hr_wsmax',\n",
    "       'mean_hr_wsgust', 'max_hr_wsmax', 'max_hr_wsgust',\n",
    "       'total_daily_sum_WSMax', 'total_daily_sum_WSGust']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wind direction\n",
    "tp_wm.loc[(tp_wm.WDMax == 0) | (tp_wm.WDMax == 360),'wind_direction_max'] = 1\n",
    "tp_wm.loc[(tp_wm.WDMax > 0) & (tp_wm.WDMax < 90),'wind_direction_max'] = 2\n",
    "tp_wm.loc[tp_wm.WDMax == 90,'wind_direction_max'] = 3\n",
    "tp_wm.loc[(tp_wm.WDMax > 90) & (tp_wm.WDMax < 180),'wind_direction_max'] = 4\n",
    "tp_wm.loc[tp_wm.WDMax == 180,'wind_direction_max'] = 5\n",
    "tp_wm.loc[(tp_wm.WDMax > 180) & (tp_wm.WDMax < 270),'wind_direction_max'] = 6\n",
    "tp_wm.loc[tp_wm.WDMax == 270,'wind_direction_max'] = 7\n",
    "tp_wm.loc[(tp_wm.WDMax > 270) & (tp_wm.WDMax < 360),'wind_direction_max'] = 8\n",
    "tp_wm.loc[tp_wm.WDMax == 550,'wind_direction_max'] = 6\n",
    "tp_wm.loc[tp_wm.WDMax == 999.9,'wind_direction_max'] = 8\n",
    "\n",
    "tp_wg.loc[(tp_wg.WDGust == 0) | (tp_wg.WDGust == 360),'wind_direction_gust'] = 1\n",
    "tp_wg.loc[(tp_wg.WDGust > 0) & (tp_wg.WDGust < 90),'wind_direction_gust'] = 2\n",
    "tp_wg.loc[tp_wg.WDGust == 90,'wind_direction_gust'] = 3\n",
    "tp_wg.loc[(tp_wg.WDGust > 90) & (tp_wg.WDGust < 180),'wind_direction_gust'] = 4\n",
    "tp_wg.loc[tp_wg.WDGust == 180,'wind_direction_gust'] = 5\n",
    "tp_wg.loc[(tp_wg.WDGust > 180) & (tp_wg.WDGust < 270),'wind_direction_gust'] = 6\n",
    "tp_wg.loc[tp_wg.WDGust == 270,'wind_direction_gust'] = 7\n",
    "tp_wg.loc[(tp_wg.WDGust > 270) & (tp_wg.WDGust < 360),'wind_direction_gust'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wind speed of each direction\n",
    "tp_wm = tp_wm.pivot_table(index=['ObsTime','typhoon','stno'],columns='wind_direction_max',values='WSMax').reset_index()\n",
    "tp_wg = tp_wg.pivot_table(index=['ObsTime','typhoon','stno'],columns='wind_direction_gust',values='WSGust').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing value\n",
    "tp_wm = tp_wm.fillna(0)\n",
    "tp_wg = tp_wg.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean/max wind speed of each direction\n",
    "mean_tp_wm = tp_wm.groupby(['typhoon','stno'],as_index=False).agg({1.0:'mean',2.0:'mean',3.0:'mean',\n",
    "                                      5.0:'mean',6.0:'mean',8.0:'mean'})\n",
    "mean_tp_wg = tp_wg.groupby(['typhoon','stno'],as_index=False).agg({1.0:'mean',2.0:'mean',3.0:'mean',\n",
    "                                      5.0:'mean',6.0:'mean',8.0:'mean'})\n",
    "max_tp_wm = tp_wm.groupby(['typhoon','stno'],as_index=False).agg({1.0:'max',2.0:'max',3.0:'max',\n",
    "                                      5.0:'max',6.0:'max',8.0:'max'})\n",
    "max_tp_wg = tp_wg.groupby(['typhoon','stno'],as_index=False).agg({1.0:'max',2.0:'max',3.0:'max',\n",
    "                                      5.0:'max',6.0:'max',8.0:'max'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename colmn\n",
    "mean_tp_wm.columns.values[2:8] = ['mean_1D_wm','mean_2D_wm','mean_3D_wm','mean_5D_wm','mean_6D_wm','mean_8D_wm']\n",
    "mean_tp_wg.columns.values[2:8] = ['mean_1D_wg','mean_2D_wg','mean_3D_wg','mean_5D_wg','mean_6D_wg','mean_8D_wg']\n",
    "max_tp_wm.columns.values[2:8] = ['max_1D_wm','max_2D_wm','max_3D_wm','max_5D_wm','max_6D_wm','max_8D_wm']\n",
    "max_tp_wg.columns.values[2:8] = ['max_1D_wg','max_2D_wg','max_3D_wg','max_5D_wg','max_6D_wg','max_8D_wg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.merge(df3, mean_tp_wm, on=['stno','typhoon'],how ='left')\n",
    "df4 = pd.merge(df4, mean_tp_wg, on=['stno','typhoon'],how ='left')\n",
    "df4 = pd.merge(df4, max_tp_wm, on=['stno','typhoon'],how ='left')\n",
    "df4 = pd.merge(df4, max_tp_wg, on=['stno','typhoon'],how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing value\n",
    "df4[['mean_1D_wm',\n",
    "       'mean_2D_wm', 'mean_3D_wm', 'mean_5D_wm', 'mean_6D_wm',\n",
    "       'mean_8D_wm', 'mean_1D_wg', 'mean_2D_wg', 'mean_3D_wg',\n",
    "       'mean_5D_wg', 'mean_6D_wg', 'mean_8D_wg', 'max_1D_wm', 'max_2D_wm',\n",
    "       'max_3D_wm', 'max_5D_wm', 'max_6D_wm', 'max_8D_wm', 'max_1D_wg',\n",
    "       'max_2D_wg', 'max_3D_wg', 'max_5D_wg', 'max_6D_wg', 'max_8D_wg']] = df4[['mean_1D_wm',\n",
    "       'mean_2D_wm', 'mean_3D_wm', 'mean_5D_wm', 'mean_6D_wm',\n",
    "       'mean_8D_wm', 'mean_1D_wg', 'mean_2D_wg', 'mean_3D_wg',\n",
    "       'mean_5D_wg', 'mean_6D_wg', 'mean_8D_wg', 'max_1D_wm', 'max_2D_wm',\n",
    "       'max_3D_wm', 'max_5D_wm', 'max_6D_wm', 'max_8D_wm', 'max_1D_wg',\n",
    "       'max_2D_wg', 'max_3D_wg', 'max_5D_wg', 'max_6D_wg', 'max_8D_wg']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# mean wind direction\n",
    "mean_wdm = tp_wm.groupby(['typhoon','stno'], as_index=False).agg({'WDMax':'mean'})\n",
    "mean_wdm.columns.values[2] = 'mean_wdmax'\n",
    "mean_wdg = tp_wg.groupby(['typhoon','stno'], as_index=False).agg({'WDGust':'mean'})\n",
    "mean_wdg.columns.values[2] = 'mean_wdgust'\n",
    "\n",
    "df4 = pd.merge(df4, mean_wdm, on=['stno','typhoon'],how ='left')\n",
    "df4 = pd.merge(df4, mean_wdg, on=['stno','typhoon'],how ='left')\n",
    "df4[['mean_wdmax','mean_wdgust']] = df4[['mean_wdmax','mean_wdgust']].fillna(0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#交互作用\n",
    "tp_wind['WSD_M'] = tp_wind.WSMax *tp_wind.wind_direction_max\n",
    "tp_wind['WSD_G'] = tp_wind.WSGust * tp_wind.wind_direction_gust\n",
    "sd1 = tp_wind.groupby(['typhoon','stno'], as_index=False).agg({'WSD_M':'mean'})\n",
    "sd1.columns.values[2] = 'mean_WSDmax'\n",
    "sd2 = tp_wind.groupby(['typhoon','stno'], as_index=False).agg({'WSD_M':'max'})\n",
    "sd2.columns.values[2] = 'max_WSDmax'\n",
    "sd3 = tp_wind.groupby(['typhoon','stno'], as_index=False).agg({'WSD_G':'mean'})\n",
    "sd3.columns.values[2] = 'mean_WSDgust'\n",
    "sd4 = tp_wind.groupby(['typhoon','stno'], as_index=False).agg({'WSD_G':'mean'})\n",
    "sd4.columns.values[2] = 'max_WSDgust'\n",
    "df4 = pd.merge(df4, sd1, on=['stno','typhoon'],how ='left')\n",
    "df4 = pd.merge(df4, sd2, on=['stno','typhoon'],how ='left')\n",
    "df4 = pd.merge(df4, sd3, on=['stno','typhoon'],how ='left')\n",
    "df4 = pd.merge(df4, sd4, on=['stno','typhoon'],how ='left')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential landslide river to city\n",
    "City_danger_river_num = pd.read_csv(path +'City_danger_river_num.csv')\n",
    "df4 = pd.merge(df4, City_danger_river_num, how='left', on='CityName')\n",
    "\n",
    "# impute missing value\n",
    "df5 = df4.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region clustering by lat-lon\n",
    "kmeans = KMeans(n_clusters=20, random_state=2017).fit(df5[['vil_lon','vil_lat']])\n",
    "df5['region_cluster'] = kmeans.predict(df5[['vil_lon','vil_lat']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of landslide alert by typhoon\n",
    "landslide = pd.read_csv(path+'landslide_alert.csv' )\n",
    "df6 = pd.merge(df5, landslide, how='left', on=['typhoon','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.to_csv(path +'df.csv',encoding='utf-8',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
